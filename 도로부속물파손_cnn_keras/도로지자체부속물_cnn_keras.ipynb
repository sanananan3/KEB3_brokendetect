{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 377
    },
    "id": "XOXAe-oxRhaP",
    "outputId": "0fc97961-67b1-4fea-b712-1e5fe96c1bfc"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import numpy as np\n",
    "from keras.preprocessing import image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "LhpA1iV9Rrhh"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.16.2'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "1tILawUeRuik"
   },
   "outputs": [],
   "source": [
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale = 1./255, # 신경망이 이미지 데이터 픽셀값 0부터 255 값을 잘 처리 못할 수도 있어서 0부터 1 사이의 값으로 만들기 위해서 255로 나눠준 것이다.\n",
    "    shear_range = 0.2, # shear (전단) 변환 : 이미지를 일정 각도로 밀어 왜곡하는 변환이다. x축에서 밀수도 있고 y축에서 밀 수도 있다.\n",
    "    zoom_range = 0.2,\n",
    "    horizontal_flip = True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "RahiCMqLRvRH"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 854 images belonging to 9 classes.\n"
     ]
    }
   ],
   "source": [
    "train_set = train_datagen.flow_from_directory ( # flow_from_directory 함수는 디렉토리 구조에서 이미지 로드한다.\n",
    "    'dataset/training_set',\n",
    "    target_size=(64,64),\n",
    "    batch_size = 32,\n",
    "    class_mode = 'categorical' # 다중 분류를 위한 categorical 모드\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "6cqhkgFuRwKc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 369 images belonging to 9 classes.\n"
     ]
    }
   ],
   "source": [
    "test_datagen = ImageDataGenerator(rescale= 1./255)\n",
    "test_set = test_datagen.flow_from_directory(\n",
    "    'dataset/test_set',\n",
    "    target_size = (64,64),\n",
    "    batch_size = 32,\n",
    "    class_mode = 'categorical'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "CNvuC8xiRxj6"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "cnn = tf.keras.models.Sequential()\n",
    "# Sequential 모델은 층을 순차적으로 쌓아 올리는 단순한 모델 구조를 정의할 때 사용된다.\n",
    "# 순차적으로 층을 추가하고, 데이터가 각 층을 통과하며 변환되도록 한다.\n",
    "\n",
    "cnn.add(tf.keras.layers.Conv2D(filters=32, kernel_size =3, activation = 'relu', input_shape=(64,64,3)))\n",
    "cnn.add(tf.keras.layers.MaxPool2D(pool_size = 2, strides=2))\n",
    "\n",
    "cnn.add(tf.keras.layers.Conv2D(filters=32, kernel_size = 3, activation = 'relu'))\n",
    "cnn.add(tf.keras.layers.MaxPool2D(pool_size=2, strides=2))\n",
    "\n",
    "cnn.add(tf.keras.layers.Flatten())\n",
    "\n",
    "cnn.add(tf.keras.layers.Dense(units=128, activation='relu'))\n",
    "cnn.add(tf.keras.layers.Dense(units=9, activation='softmax')) # 클래스 수에 맞게 유닛 수를 9로 설정하고 softmax activate function 활성화 함수 사용\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "Bg7Js9y3R1TU",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 1s/step - accuracy: 0.2313 - loss: 2.0734 - val_accuracy: 0.2249 - val_loss: 2.0067\n",
      "Epoch 2/60\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 986ms/step - accuracy: 0.3334 - loss: 1.9131 - val_accuracy: 0.2900 - val_loss: 1.9901\n",
      "Epoch 3/60\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 972ms/step - accuracy: 0.4059 - loss: 1.7006 - val_accuracy: 0.2791 - val_loss: 2.2298\n",
      "Epoch 4/60\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 965ms/step - accuracy: 0.5064 - loss: 1.5165 - val_accuracy: 0.3008 - val_loss: 2.2688\n",
      "Epoch 5/60\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 959ms/step - accuracy: 0.5548 - loss: 1.3422 - val_accuracy: 0.3523 - val_loss: 2.1249\n",
      "Epoch 6/60\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 955ms/step - accuracy: 0.5611 - loss: 1.2826 - val_accuracy: 0.3306 - val_loss: 2.5851\n",
      "Epoch 7/60\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 952ms/step - accuracy: 0.6425 - loss: 1.0610 - val_accuracy: 0.3415 - val_loss: 2.6462\n",
      "Epoch 8/60\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 958ms/step - accuracy: 0.6451 - loss: 0.9902 - val_accuracy: 0.2602 - val_loss: 2.9645\n",
      "Epoch 9/60\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 930ms/step - accuracy: 0.6557 - loss: 1.0156 - val_accuracy: 0.3415 - val_loss: 2.7130\n",
      "Epoch 10/60\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 928ms/step - accuracy: 0.7257 - loss: 0.8362 - val_accuracy: 0.2737 - val_loss: 2.9078\n",
      "Epoch 11/60\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 961ms/step - accuracy: 0.7615 - loss: 0.7471 - val_accuracy: 0.3577 - val_loss: 2.7234\n",
      "Epoch 12/60\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 965ms/step - accuracy: 0.7502 - loss: 0.7806 - val_accuracy: 0.3496 - val_loss: 3.3388\n",
      "Epoch 13/60\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 959ms/step - accuracy: 0.7920 - loss: 0.6269 - val_accuracy: 0.3035 - val_loss: 3.0447\n",
      "Epoch 14/60\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 941ms/step - accuracy: 0.8174 - loss: 0.6090 - val_accuracy: 0.3523 - val_loss: 3.1855\n",
      "Epoch 15/60\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 969ms/step - accuracy: 0.8180 - loss: 0.5876 - val_accuracy: 0.3577 - val_loss: 3.3266\n",
      "Epoch 16/60\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 955ms/step - accuracy: 0.8437 - loss: 0.4908 - val_accuracy: 0.3550 - val_loss: 2.9610\n",
      "Epoch 17/60\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 937ms/step - accuracy: 0.8172 - loss: 0.5090 - val_accuracy: 0.3062 - val_loss: 3.6545\n",
      "Epoch 18/60\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 993ms/step - accuracy: 0.8390 - loss: 0.4883 - val_accuracy: 0.3523 - val_loss: 3.1996\n",
      "Epoch 19/60\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 981ms/step - accuracy: 0.8520 - loss: 0.4346 - val_accuracy: 0.3794 - val_loss: 3.3547\n",
      "Epoch 20/60\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 968ms/step - accuracy: 0.8894 - loss: 0.3630 - val_accuracy: 0.3469 - val_loss: 3.6631\n",
      "Epoch 21/60\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 963ms/step - accuracy: 0.8965 - loss: 0.3831 - val_accuracy: 0.2466 - val_loss: 4.2275\n",
      "Epoch 22/60\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 1s/step - accuracy: 0.8743 - loss: 0.4213 - val_accuracy: 0.3631 - val_loss: 3.7166\n",
      "Epoch 23/60\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 974ms/step - accuracy: 0.9057 - loss: 0.3028 - val_accuracy: 0.3767 - val_loss: 3.7878\n",
      "Epoch 24/60\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 981ms/step - accuracy: 0.8821 - loss: 0.3563 - val_accuracy: 0.3496 - val_loss: 4.0959\n",
      "Epoch 25/60\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 1s/step - accuracy: 0.8898 - loss: 0.3151 - val_accuracy: 0.3252 - val_loss: 4.3427\n",
      "Epoch 26/60\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 1s/step - accuracy: 0.8895 - loss: 0.3198 - val_accuracy: 0.3306 - val_loss: 3.8466\n",
      "Epoch 27/60\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 969ms/step - accuracy: 0.9013 - loss: 0.3172 - val_accuracy: 0.3794 - val_loss: 3.8254\n",
      "Epoch 28/60\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 968ms/step - accuracy: 0.9115 - loss: 0.2689 - val_accuracy: 0.3225 - val_loss: 4.0696\n",
      "Epoch 29/60\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 946ms/step - accuracy: 0.9072 - loss: 0.2699 - val_accuracy: 0.3930 - val_loss: 4.1845\n",
      "Epoch 30/60\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 972ms/step - accuracy: 0.9327 - loss: 0.2421 - val_accuracy: 0.3848 - val_loss: 4.2068\n",
      "Epoch 31/60\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 995ms/step - accuracy: 0.9311 - loss: 0.2144 - val_accuracy: 0.3957 - val_loss: 4.0152\n",
      "Epoch 32/60\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 1s/step - accuracy: 0.9268 - loss: 0.1954 - val_accuracy: 0.3035 - val_loss: 4.9598\n",
      "Epoch 33/60\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 1s/step - accuracy: 0.9116 - loss: 0.2580 - val_accuracy: 0.3198 - val_loss: 4.5652\n",
      "Epoch 34/60\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 1s/step - accuracy: 0.9352 - loss: 0.2196 - val_accuracy: 0.3659 - val_loss: 4.0766\n",
      "Epoch 35/60\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 962ms/step - accuracy: 0.9584 - loss: 0.1562 - val_accuracy: 0.3333 - val_loss: 4.9400\n",
      "Epoch 36/60\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 971ms/step - accuracy: 0.9412 - loss: 0.1763 - val_accuracy: 0.3740 - val_loss: 4.3947\n",
      "Epoch 37/60\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 990ms/step - accuracy: 0.9141 - loss: 0.2752 - val_accuracy: 0.3415 - val_loss: 4.4217\n",
      "Epoch 38/60\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 1s/step - accuracy: 0.9253 - loss: 0.2248 - val_accuracy: 0.3957 - val_loss: 4.9075\n",
      "Epoch 39/60\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 953ms/step - accuracy: 0.9494 - loss: 0.1805 - val_accuracy: 0.3740 - val_loss: 4.4710\n",
      "Epoch 40/60\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 977ms/step - accuracy: 0.9493 - loss: 0.1789 - val_accuracy: 0.3089 - val_loss: 5.1967\n",
      "Epoch 41/60\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 1s/step - accuracy: 0.9419 - loss: 0.1729 - val_accuracy: 0.3496 - val_loss: 4.9324\n",
      "Epoch 42/60\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 1s/step - accuracy: 0.9479 - loss: 0.1530 - val_accuracy: 0.3631 - val_loss: 4.7099\n",
      "Epoch 43/60\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 970ms/step - accuracy: 0.9378 - loss: 0.1583 - val_accuracy: 0.3631 - val_loss: 4.4775\n",
      "Epoch 44/60\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 1s/step - accuracy: 0.9476 - loss: 0.1562 - val_accuracy: 0.3659 - val_loss: 4.9583\n",
      "Epoch 45/60\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 1s/step - accuracy: 0.9511 - loss: 0.1473 - val_accuracy: 0.3469 - val_loss: 5.0744\n",
      "Epoch 46/60\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 1s/step - accuracy: 0.9487 - loss: 0.1597 - val_accuracy: 0.3767 - val_loss: 4.6476\n",
      "Epoch 47/60\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 996ms/step - accuracy: 0.9515 - loss: 0.1298 - val_accuracy: 0.3631 - val_loss: 5.4100\n",
      "Epoch 48/60\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 960ms/step - accuracy: 0.9618 - loss: 0.1023 - val_accuracy: 0.3496 - val_loss: 5.4213\n",
      "Epoch 49/60\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 1s/step - accuracy: 0.9590 - loss: 0.1434 - val_accuracy: 0.3225 - val_loss: 4.9226\n",
      "Epoch 50/60\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 981ms/step - accuracy: 0.9621 - loss: 0.1448 - val_accuracy: 0.3902 - val_loss: 5.0535\n",
      "Epoch 51/60\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 953ms/step - accuracy: 0.9488 - loss: 0.1223 - val_accuracy: 0.3333 - val_loss: 5.2681\n",
      "Epoch 52/60\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 961ms/step - accuracy: 0.9531 - loss: 0.1338 - val_accuracy: 0.3875 - val_loss: 5.0976\n",
      "Epoch 53/60\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 967ms/step - accuracy: 0.9644 - loss: 0.1101 - val_accuracy: 0.3631 - val_loss: 5.3735\n",
      "Epoch 54/60\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 1000ms/step - accuracy: 0.9670 - loss: 0.1104 - val_accuracy: 0.3686 - val_loss: 5.1969\n",
      "Epoch 55/60\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 1000ms/step - accuracy: 0.9645 - loss: 0.1157 - val_accuracy: 0.3550 - val_loss: 5.8443\n",
      "Epoch 56/60\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 980ms/step - accuracy: 0.9617 - loss: 0.1139 - val_accuracy: 0.3523 - val_loss: 5.9106\n",
      "Epoch 57/60\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 951ms/step - accuracy: 0.9719 - loss: 0.1171 - val_accuracy: 0.3577 - val_loss: 5.3459\n",
      "Epoch 58/60\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 944ms/step - accuracy: 0.9521 - loss: 0.1260 - val_accuracy: 0.4011 - val_loss: 4.5420\n",
      "Epoch 59/60\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 980ms/step - accuracy: 0.9623 - loss: 0.1295 - val_accuracy: 0.3388 - val_loss: 5.4771\n",
      "Epoch 60/60\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 970ms/step - accuracy: 0.9540 - loss: 0.1368 - val_accuracy: 0.3740 - val_loss: 4.6691\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x236f9e6d100>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Adam 옵티마이저에 학습률 지정\n",
    "\n",
    "learning_rate = 0.0008\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate = learning_rate)\n",
    "\n",
    "cnn.compile(optimizer= optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "cnn.fit( x = train_set, validation_data = test_set, epochs=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "1fKaf70pR2zl"
   },
   "outputs": [],
   "source": [
    "\n",
    "# 테스트 이미지 예측 (class 9개 모두 다 해봐야 함)\n",
    "test_image1 = image.load_img ('dataset/single_prediction/PE드럼_파손.jpg', target_size=(64,64))\n",
    "\n",
    "\n",
    "# PIL (Python Imaging Libary) 이미지 인스턴스 = 이미지 데이터를 다루기 위한 python 객체\n",
    "# PIL을 넘파이 배열로 변환하기 왜냐? 신경망 모델 자체가 넘파이 배열을 입력으로 받기 때문이다~ PIL 이미지를 넘파이 배열로 변환해야 한다.\n",
    "\n",
    "\n",
    "test_image1 = image.img_to_array(test_image1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "epOcCt9DR4a3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "{'PE드럼 파손': 0, 'PE방호벽 파손': 1, 'PE안내봉 파손': 2, 'PE입간판 파손': 3, 'PE휀스 파손': 4, '도로 갈라짐': 5, '라바콘 파손': 6, '시선유도봉 파손': 7, '제설함 파손': 8}\n",
      "PE드럼 파손\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 배열에 새로운 차원을 추가한다.\n",
    "# 딥러닝 프레임워크에서 모델이 입력 데이터를 처리하는 방식이 그렇다. keras, tensorflow, pytorch 등등 모두 데이터를 배치 단위로 처리한다.\n",
    "# 4차원 입력 배열의 필요성\n",
    "# 딥러닝 모델에 입력되는 데이터의 형식 : (배치크기, 높이, 너비, 채널수)\n",
    "# 배치 처리를 위해서 배열을 4차원으로 만들기 위해 사용된다. (배치 크기, 높이, 너비, 채널 수)\n",
    "\n",
    "test_image1 = np.expand_dims(test_image1, axis=0)\n",
    "\n",
    "result1 = cnn.predict(test_image1/255.0) # test image도 픽셀 값이 0부터 255 사이의 값이기 때문에, 이걸 정규화 해줘야 한다. 0부터 1 사이의 값으로 정규화 실시한다.\n",
    "# result = (1,9) 1개의 이미지와 9개의 클래스\n",
    "\n",
    "# 클래스 인덱스 확인하기\n",
    "\n",
    "print(train_set.class_indices)  # class_indices 속성: 클래스 레이블과 디렉토리 이름 사이의 매핑을 반환한다.\n",
    "\n",
    "# 예측 결과 출력하기\n",
    "\n",
    "predicted_class = np.argmax(result1, axis=1) # 하나의 요소를 가진 배열이 된다.\n",
    "class_labels = list(train_set.class_indices.keys()) # 딕셔너리의 key (pe드럼 파손, pe휀스 파손 등등 ... ) 만 추출해서 list 로 변환한다.\n",
    "prediction = class_labels[predicted_class[0]] # predicted_class의 첫번째이자 유일한 요소를 가져온다.\n",
    "\n",
    "print(prediction)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "xM86fkbWR7qW"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "{'PE드럼 파손': 0, 'PE방호벽 파손': 1, 'PE안내봉 파손': 2, 'PE입간판 파손': 3, 'PE휀스 파손': 4, '도로 갈라짐': 5, '라바콘 파손': 6, '시선유도봉 파손': 7, '제설함 파손': 8}\n",
      "PE방호벽 파손\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 테스트 이미지 예측 (class 9개 모두 다 해봐야 함)\n",
    "test_image1 = image.load_img ('dataset/single_prediction/PE방호벽_파손.jpg', target_size=(64,64))\n",
    "\n",
    "\n",
    "# PIL (Python Imaging Libary) 이미지 인스턴스 = 이미지 데이터를 다루기 위한 python 객체\n",
    "# PIL을 넘파이 배열로 변환하기 왜냐? 신경망 모델 자체가 넘파이 배열을 입력으로 받기 때문이다~ PIL 이미지를 넘파이 배열로 변환해야 한다.\n",
    "\n",
    "\n",
    "test_image1 = image.img_to_array(test_image1)\n",
    "\n",
    "\n",
    "# 배열에 새로운 차원을 추가한다.\n",
    "# 딥러닝 프레임워크에서 모델이 입력 데이터를 처리하는 방식이 그렇다. keras, tensorflow, pytorch 등등 모두 데이터를 배치 단위로 처리한다.\n",
    "# 4차원 입력 배열의 필요성\n",
    "# 딥러닝 모델에 입력되는 데이터의 형식 : (배치크기, 높이, 너비, 채널수)\n",
    "# 배치 처리를 위해서 배열을 4차원으로 만들기 위해 사용된다. (배치 크기, 높이, 너비, 채널 수)\n",
    "\n",
    "test_image1 = np.expand_dims(test_image1, axis=0)\n",
    "\n",
    "result1 = cnn.predict(test_image1/255.0) # test image도 픽셀 값이 0부터 255 사이의 값이기 때문에, 이걸 정규화 해줘야 한다. 0부터 1 사이의 값으로 정규화 실시한다.\n",
    "# result = (1,9) 1개의 이미지와 9개의 클래스\n",
    "\n",
    "# 클래스 인덱스 확인하기\n",
    "\n",
    "print(train_set.class_indices)  # class_indices 속성: 클래스 레이블과 디렉토리 이름 사이의 매핑을 반환한다.\n",
    "\n",
    "# 예측 결과 출력하기\n",
    "\n",
    "predicted_class = np.argmax(result1, axis=1) # 하나의 요소를 가진 배열이 된다.\n",
    "class_labels = list(train_set.class_indices.keys()) # 딕셔너리의 key (pe드럼 파손, pe휀스 파손 등등 ... ) 만 추출해서 list 로 변환한다.\n",
    "prediction = class_labels[predicted_class[0]] # predicted_class의 첫번째이자 유일한 요소를 가져온다.\n",
    "\n",
    "print(prediction)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "{'PE드럼 파손': 0, 'PE방호벽 파손': 1, 'PE안내봉 파손': 2, 'PE입간판 파손': 3, 'PE휀스 파손': 4, '도로 갈라짐': 5, '라바콘 파손': 6, '시선유도봉 파손': 7, '제설함 파손': 8}\n",
      "PE안내봉 파손\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 테스트 이미지 예측 (class 9개 모두 다 해봐야 함)\n",
    "test_image1 = image.load_img ('dataset/single_prediction/PE안내봉_파손.jpg', target_size=(64,64))\n",
    "\n",
    "\n",
    "# PIL (Python Imaging Libary) 이미지 인스턴스 = 이미지 데이터를 다루기 위한 python 객체\n",
    "# PIL을 넘파이 배열로 변환하기 왜냐? 신경망 모델 자체가 넘파이 배열을 입력으로 받기 때문이다~ PIL 이미지를 넘파이 배열로 변환해야 한다.\n",
    "\n",
    "\n",
    "test_image1 = image.img_to_array(test_image1)\n",
    "\n",
    "\n",
    "# 배열에 새로운 차원을 추가한다.\n",
    "# 딥러닝 프레임워크에서 모델이 입력 데이터를 처리하는 방식이 그렇다. keras, tensorflow, pytorch 등등 모두 데이터를 배치 단위로 처리한다.\n",
    "# 4차원 입력 배열의 필요성\n",
    "# 딥러닝 모델에 입력되는 데이터의 형식 : (배치크기, 높이, 너비, 채널수)\n",
    "# 배치 처리를 위해서 배열을 4차원으로 만들기 위해 사용된다. (배치 크기, 높이, 너비, 채널 수)\n",
    "\n",
    "test_image1 = np.expand_dims(test_image1, axis=0)\n",
    "\n",
    "result1 = cnn.predict(test_image1/255.0) # test image도 픽셀 값이 0부터 255 사이의 값이기 때문에, 이걸 정규화 해줘야 한다. 0부터 1 사이의 값으로 정규화 실시한다.\n",
    "# result = (1,9) 1개의 이미지와 9개의 클래스\n",
    "\n",
    "# 클래스 인덱스 확인하기\n",
    "\n",
    "print(train_set.class_indices)  # class_indices 속성: 클래스 레이블과 디렉토리 이름 사이의 매핑을 반환한다.\n",
    "\n",
    "# 예측 결과 출력하기\n",
    "\n",
    "predicted_class = np.argmax(result1, axis=1) # 하나의 요소를 가진 배열이 된다.\n",
    "class_labels = list(train_set.class_indices.keys()) # 딕셔너리의 key (pe드럼 파손, pe휀스 파손 등등 ... ) 만 추출해서 list 로 변환한다.\n",
    "prediction = class_labels[predicted_class[0]] # predicted_class의 첫번째이자 유일한 요소를 가져온다.\n",
    "\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "{'PE드럼 파손': 0, 'PE방호벽 파손': 1, 'PE안내봉 파손': 2, 'PE입간판 파손': 3, 'PE휀스 파손': 4, '도로 갈라짐': 5, '라바콘 파손': 6, '시선유도봉 파손': 7, '제설함 파손': 8}\n",
      "도로 갈라짐\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 테스트 이미지 예측 (class 9개 모두 다 해봐야 함)\n",
    "test_image1 = image.load_img ('dataset/single_prediction/도로_갈라짐.jpg', target_size=(64,64))\n",
    "\n",
    "\n",
    "# PIL (Python Imaging Libary) 이미지 인스턴스 = 이미지 데이터를 다루기 위한 python 객체\n",
    "# PIL을 넘파이 배열로 변환하기 왜냐? 신경망 모델 자체가 넘파이 배열을 입력으로 받기 때문이다~ PIL 이미지를 넘파이 배열로 변환해야 한다.\n",
    "\n",
    "\n",
    "test_image1 = image.img_to_array(test_image1)\n",
    "\n",
    "\n",
    "# 배열에 새로운 차원을 추가한다.\n",
    "# 딥러닝 프레임워크에서 모델이 입력 데이터를 처리하는 방식이 그렇다. keras, tensorflow, pytorch 등등 모두 데이터를 배치 단위로 처리한다.\n",
    "# 4차원 입력 배열의 필요성\n",
    "# 딥러닝 모델에 입력되는 데이터의 형식 : (배치크기, 높이, 너비, 채널수)\n",
    "# 배치 처리를 위해서 배열을 4차원으로 만들기 위해 사용된다. (배치 크기, 높이, 너비, 채널 수)\n",
    "\n",
    "test_image1 = np.expand_dims(test_image1, axis=0)\n",
    "\n",
    "result1 = cnn.predict(test_image1/255.0) # test image도 픽셀 값이 0부터 255 사이의 값이기 때문에, 이걸 정규화 해줘야 한다. 0부터 1 사이의 값으로 정규화 실시한다.\n",
    "# result = (1,9) 1개의 이미지와 9개의 클래스\n",
    "\n",
    "# 클래스 인덱스 확인하기\n",
    "\n",
    "print(train_set.class_indices)  # class_indices 속성: 클래스 레이블과 디렉토리 이름 사이의 매핑을 반환한다.\n",
    "\n",
    "# 예측 결과 출력하기\n",
    "\n",
    "predicted_class = np.argmax(result1, axis=1) # 하나의 요소를 가진 배열이 된다.\n",
    "class_labels = list(train_set.class_indices.keys()) # 딕셔너리의 key (pe드럼 파손, pe휀스 파손 등등 ... ) 만 추출해서 list 로 변환한다.\n",
    "prediction = class_labels[predicted_class[0]] # predicted_class의 첫번째이자 유일한 요소를 가져온다.\n",
    "\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "{'PE드럼 파손': 0, 'PE방호벽 파손': 1, 'PE안내봉 파손': 2, 'PE입간판 파손': 3, 'PE휀스 파손': 4, '도로 갈라짐': 5, '라바콘 파손': 6, '시선유도봉 파손': 7, '제설함 파손': 8}\n",
      "라바콘 파손\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 테스트 이미지 예측 (class 9개 모두 다 해봐야 함)\n",
    "test_image1 = image.load_img ('dataset/single_prediction/라바콘_파손.jpg', target_size=(64,64))\n",
    "\n",
    "\n",
    "# PIL (Python Imaging Libary) 이미지 인스턴스 = 이미지 데이터를 다루기 위한 python 객체\n",
    "# PIL을 넘파이 배열로 변환하기 왜냐? 신경망 모델 자체가 넘파이 배열을 입력으로 받기 때문이다~ PIL 이미지를 넘파이 배열로 변환해야 한다.\n",
    "\n",
    "\n",
    "test_image1 = image.img_to_array(test_image1)\n",
    "\n",
    "\n",
    "# 배열에 새로운 차원을 추가한다.\n",
    "# 딥러닝 프레임워크에서 모델이 입력 데이터를 처리하는 방식이 그렇다. keras, tensorflow, pytorch 등등 모두 데이터를 배치 단위로 처리한다.\n",
    "# 4차원 입력 배열의 필요성\n",
    "# 딥러닝 모델에 입력되는 데이터의 형식 : (배치크기, 높이, 너비, 채널수)\n",
    "# 배치 처리를 위해서 배열을 4차원으로 만들기 위해 사용된다. (배치 크기, 높이, 너비, 채널 수)\n",
    "\n",
    "test_image1 = np.expand_dims(test_image1, axis=0)\n",
    "\n",
    "result1 = cnn.predict(test_image1/255.0) # test image도 픽셀 값이 0부터 255 사이의 값이기 때문에, 이걸 정규화 해줘야 한다. 0부터 1 사이의 값으로 정규화 실시한다.\n",
    "# result = (1,9) 1개의 이미지와 9개의 클래스\n",
    "\n",
    "# 클래스 인덱스 확인하기\n",
    "\n",
    "print(train_set.class_indices)  # class_indices 속성: 클래스 레이블과 디렉토리 이름 사이의 매핑을 반환한다.\n",
    "\n",
    "# 예측 결과 출력하기\n",
    "\n",
    "predicted_class = np.argmax(result1, axis=1) # 하나의 요소를 가진 배열이 된다.\n",
    "class_labels = list(train_set.class_indices.keys()) # 딕셔너리의 key (pe드럼 파손, pe휀스 파손 등등 ... ) 만 추출해서 list 로 변환한다.\n",
    "prediction = class_labels[predicted_class[0]] # predicted_class의 첫번째이자 유일한 요소를 가져온다.\n",
    "\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "{'PE드럼 파손': 0, 'PE방호벽 파손': 1, 'PE안내봉 파손': 2, 'PE입간판 파손': 3, 'PE휀스 파손': 4, '도로 갈라짐': 5, '라바콘 파손': 6, '시선유도봉 파손': 7, '제설함 파손': 8}\n",
      "시선유도봉 파손\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 테스트 이미지 예측 (class 9개 모두 다 해봐야 함)\n",
    "test_image1 = image.load_img ('dataset/single_prediction/시선유도봉_파손.jpg', target_size=(64,64))\n",
    "\n",
    "\n",
    "# PIL (Python Imaging Libary) 이미지 인스턴스 = 이미지 데이터를 다루기 위한 python 객체\n",
    "# PIL을 넘파이 배열로 변환하기 왜냐? 신경망 모델 자체가 넘파이 배열을 입력으로 받기 때문이다~ PIL 이미지를 넘파이 배열로 변환해야 한다.\n",
    "\n",
    "\n",
    "test_image1 = image.img_to_array(test_image1)\n",
    "\n",
    "\n",
    "# 배열에 새로운 차원을 추가한다.\n",
    "# 딥러닝 프레임워크에서 모델이 입력 데이터를 처리하는 방식이 그렇다. keras, tensorflow, pytorch 등등 모두 데이터를 배치 단위로 처리한다.\n",
    "# 4차원 입력 배열의 필요성\n",
    "# 딥러닝 모델에 입력되는 데이터의 형식 : (배치크기, 높이, 너비, 채널수)\n",
    "# 배치 처리를 위해서 배열을 4차원으로 만들기 위해 사용된다. (배치 크기, 높이, 너비, 채널 수)\n",
    "\n",
    "test_image1 = np.expand_dims(test_image1, axis=0)\n",
    "\n",
    "result1 = cnn.predict(test_image1/255.0) # test image도 픽셀 값이 0부터 255 사이의 값이기 때문에, 이걸 정규화 해줘야 한다. 0부터 1 사이의 값으로 정규화 실시한다.\n",
    "# result = (1,9) 1개의 이미지와 9개의 클래스\n",
    "\n",
    "# 클래스 인덱스 확인하기\n",
    "\n",
    "print(train_set.class_indices)  # class_indices 속성: 클래스 레이블과 디렉토리 이름 사이의 매핑을 반환한다.\n",
    "\n",
    "# 예측 결과 출력하기\n",
    "\n",
    "predicted_class = np.argmax(result1, axis=1) # 하나의 요소를 가진 배열이 된다.\n",
    "class_labels = list(train_set.class_indices.keys()) # 딕셔너리의 key (pe드럼 파손, pe휀스 파손 등등 ... ) 만 추출해서 list 로 변환한다.\n",
    "prediction = class_labels[predicted_class[0]] # predicted_class의 첫번째이자 유일한 요소를 가져온다.\n",
    "\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "{'PE드럼 파손': 0, 'PE방호벽 파손': 1, 'PE안내봉 파손': 2, 'PE입간판 파손': 3, 'PE휀스 파손': 4, '도로 갈라짐': 5, '라바콘 파손': 6, '시선유도봉 파손': 7, '제설함 파손': 8}\n",
      "제설함 파손\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 테스트 이미지 예측 (class 9개 모두 다 해봐야 함)\n",
    "test_image1 = image.load_img ('dataset/single_prediction/제설함_파손.jpg', target_size=(64,64))\n",
    "\n",
    "\n",
    "# PIL (Python Imaging Libary) 이미지 인스턴스 = 이미지 데이터를 다루기 위한 python 객체\n",
    "# PIL을 넘파이 배열로 변환하기 왜냐? 신경망 모델 자체가 넘파이 배열을 입력으로 받기 때문이다~ PIL 이미지를 넘파이 배열로 변환해야 한다.\n",
    "\n",
    "\n",
    "test_image1 = image.img_to_array(test_image1)\n",
    "\n",
    "\n",
    "# 배열에 새로운 차원을 추가한다.\n",
    "# 딥러닝 프레임워크에서 모델이 입력 데이터를 처리하는 방식이 그렇다. keras, tensorflow, pytorch 등등 모두 데이터를 배치 단위로 처리한다.\n",
    "# 4차원 입력 배열의 필요성\n",
    "# 딥러닝 모델에 입력되는 데이터의 형식 : (배치크기, 높이, 너비, 채널수)\n",
    "# 배치 처리를 위해서 배열을 4차원으로 만들기 위해 사용된다. (배치 크기, 높이, 너비, 채널 수)\n",
    "\n",
    "test_image1 = np.expand_dims(test_image1, axis=0)\n",
    "\n",
    "result1 = cnn.predict(test_image1/255.0) # test image도 픽셀 값이 0부터 255 사이의 값이기 때문에, 이걸 정규화 해줘야 한다. 0부터 1 사이의 값으로 정규화 실시한다.\n",
    "# result = (1,9) 1개의 이미지와 9개의 클래스\n",
    "\n",
    "# 클래스 인덱스 확인하기\n",
    "\n",
    "print(train_set.class_indices)  # class_indices 속성: 클래스 레이블과 디렉토리 이름 사이의 매핑을 반환한다.\n",
    "\n",
    "# 예측 결과 출력하기\n",
    "\n",
    "predicted_class = np.argmax(result1, axis=1) # 하나의 요소를 가진 배열이 된다.\n",
    "class_labels = list(train_set.class_indices.keys()) # 딕셔너리의 key (pe드럼 파손, pe휀스 파손 등등 ... ) 만 추출해서 list 로 변환한다.\n",
    "prediction = class_labels[predicted_class[0]] # predicted_class의 첫번째이자 유일한 요소를 가져온다.\n",
    "\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "{'PE드럼 파손': 0, 'PE방호벽 파손': 1, 'PE안내봉 파손': 2, 'PE입간판 파손': 3, 'PE휀스 파손': 4, '도로 갈라짐': 5, '라바콘 파손': 6, '시선유도봉 파손': 7, '제설함 파손': 8}\n",
      "PE입간판 파손\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 테스트 이미지 예측 (class 9개 모두 다 해봐야 함)\n",
    "test_image1 = image.load_img ('dataset/single_prediction/PE입간판_파손.png', target_size=(64,64))\n",
    "\n",
    "\n",
    "# PIL (Python Imaging Libary) 이미지 인스턴스 = 이미지 데이터를 다루기 위한 python 객체\n",
    "# PIL을 넘파이 배열로 변환하기 왜냐? 신경망 모델 자체가 넘파이 배열을 입력으로 받기 때문이다~ PIL 이미지를 넘파이 배열로 변환해야 한다.\n",
    "\n",
    "\n",
    "test_image1 = image.img_to_array(test_image1)\n",
    "\n",
    "\n",
    "# 배열에 새로운 차원을 추가한다.\n",
    "# 딥러닝 프레임워크에서 모델이 입력 데이터를 처리하는 방식이 그렇다. keras, tensorflow, pytorch 등등 모두 데이터를 배치 단위로 처리한다.\n",
    "# 4차원 입력 배열의 필요성\n",
    "# 딥러닝 모델에 입력되는 데이터의 형식 : (배치크기, 높이, 너비, 채널수)\n",
    "# 배치 처리를 위해서 배열을 4차원으로 만들기 위해 사용된다. (배치 크기, 높이, 너비, 채널 수)\n",
    "\n",
    "test_image1 = np.expand_dims(test_image1, axis=0)\n",
    "\n",
    "result1 = cnn.predict(test_image1/255.0) # test image도 픽셀 값이 0부터 255 사이의 값이기 때문에, 이걸 정규화 해줘야 한다. 0부터 1 사이의 값으로 정규화 실시한다.\n",
    "# result = (1,9) 1개의 이미지와 9개의 클래스\n",
    "\n",
    "# 클래스 인덱스 확인하기\n",
    "\n",
    "print(train_set.class_indices)  # class_indices 속성: 클래스 레이블과 디렉토리 이름 사이의 매핑을 반환한다.\n",
    "\n",
    "# 예측 결과 출력하기\n",
    "\n",
    "predicted_class = np.argmax(result1, axis=1) # 하나의 요소를 가진 배열이 된다.\n",
    "class_labels = list(train_set.class_indices.keys()) # 딕셔너리의 key (pe드럼 파손, pe휀스 파손 등등 ... ) 만 추출해서 list 로 변환한다.\n",
    "prediction = class_labels[predicted_class[0]] # predicted_class의 첫번째이자 유일한 요소를 가져온다.\n",
    "\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in c:\\users\\user\\anaconda3\\lib\\site-packages (2.16.2)\n",
      "Requirement already satisfied: keras in c:\\users\\user\\anaconda3\\lib\\site-packages (3.4.1)\n",
      "Requirement already satisfied: tensorflow-intel==2.16.2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow) (2.16.2)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.2->tensorflow) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.2->tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=23.5.26 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.2->tensorflow) (24.3.25)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.2->tensorflow) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.2->tensorflow) (0.2.0)\n",
      "Requirement already satisfied: h5py>=3.10.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.2->tensorflow) (3.11.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.2->tensorflow) (18.1.1)\n",
      "Requirement already satisfied: ml-dtypes~=0.3.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.2->tensorflow) (0.3.2)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.2->tensorflow) (3.3.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.2->tensorflow) (23.2)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.2->tensorflow) (3.20.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.2->tensorflow) (2.32.2)\n",
      "Requirement already satisfied: setuptools in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.2->tensorflow) (69.5.1)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.2->tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.2->tensorflow) (2.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.2->tensorflow) (4.11.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.2->tensorflow) (1.14.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.2->tensorflow) (1.64.1)\n",
      "Requirement already satisfied: tensorboard<2.17,>=2.16 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.2->tensorflow) (2.16.2)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.26.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.2->tensorflow) (1.26.4)\n",
      "Requirement already satisfied: rich in c:\\users\\user\\anaconda3\\lib\\site-packages (from keras) (13.3.5)\n",
      "Requirement already satisfied: namex in c:\\users\\user\\anaconda3\\lib\\site-packages (from keras) (0.0.8)\n",
      "Requirement already satisfied: optree in c:\\users\\user\\anaconda3\\lib\\site-packages (from keras) (0.12.1)\n",
      "Requirement already satisfied: markdown-it-py<3.0.0,>=2.2.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from rich->keras) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from rich->keras) (2.15.1)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.16.2->tensorflow) (0.43.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from markdown-it-py<3.0.0,>=2.2.0->rich->keras) (0.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.16.2->tensorflow) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.16.2->tensorflow) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.16.2->tensorflow) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.16.2->tensorflow) (2024.6.2)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorboard<2.17,>=2.16->tensorflow-intel==2.16.2->tensorflow) (3.4.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorboard<2.17,>=2.16->tensorflow-intel==2.16.2->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorboard<2.17,>=2.16->tensorflow-intel==2.16.2->tensorflow) (3.0.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.17,>=2.16->tensorflow-intel==2.16.2->tensorflow) (2.1.3)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "!pip install --upgrade tensorflow keras\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Please consider providing the trackable_obj argument in the from_concrete_functions. Providing without the trackable_obj argument is deprecated and it will use the deprecated conversion path.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "# keras 모델을 tflite (모바일에 적합한 형태) 로 변환하기 \n",
    "\n",
    "# 1 . 모델 저장\n",
    "cnn.save('cnn_krs_2.keras')\n",
    "\n",
    "# 2. 모델 로드\n",
    "cnn_keras_model = tf.keras.models.load_model('cnn_krs_2.keras')\n",
    "\n",
    "# 3. 모델의 호출 함수를 tf.function으로 래핑\n",
    "@tf.function\n",
    "def wrapped_model(inputs):\n",
    "    return cnn_keras_model(inputs)\n",
    "\n",
    "# 4. TFLiteConverter를 사용하여 모델을 TFLite 형식으로 변환\n",
    "converter = tf.lite.TFLiteConverter.from_concrete_functions([wrapped_model.get_concrete_function(tf.TensorSpec([None, 64, 64, 3], tf.float32))])\n",
    "tf_cnnMdl = converter.convert()\n",
    "\n",
    "# 5. TFLite 모델을 파일로 저장하기\n",
    "with open('cnn_keras_mdl_2.tflite', 'wb') as f:\n",
    "    f.write(tf_cnnMdl)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
