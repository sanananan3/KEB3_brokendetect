{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 377
    },
    "id": "XOXAe-oxRhaP",
    "outputId": "0fc97961-67b1-4fea-b712-1e5fe96c1bfc"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import numpy as np\n",
    "from keras.preprocessing import image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "LhpA1iV9Rrhh"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.16.2'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "1tILawUeRuik"
   },
   "outputs": [],
   "source": [
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale = 1./255, # 신경망이 이미지 데이터 픽셀값 0부터 255 값을 잘 처리 못할 수도 있어서 0부터 1 사이의 값으로 만들기 위해서 255로 나눠준 것이다.\n",
    "    shear_range = 0.2, # shear (전단) 변환 : 이미지를 일정 각도로 밀어 왜곡하는 변환이다. x축에서 밀수도 있고 y축에서 밀 수도 있다.\n",
    "    zoom_range = 0.2,\n",
    "    horizontal_flip = True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "RahiCMqLRvRH"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 854 images belonging to 9 classes.\n"
     ]
    }
   ],
   "source": [
    "train_set = train_datagen.flow_from_directory ( # flow_from_directory 함수는 디렉토리 구조에서 이미지 로드한다.\n",
    "    'dataset/training_set',\n",
    "    target_size=(64,64),\n",
    "    batch_size = 32,\n",
    "    class_mode = 'categorical' # 다중 분류를 위한 categorical 모드\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "6cqhkgFuRwKc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 369 images belonging to 9 classes.\n"
     ]
    }
   ],
   "source": [
    "test_datagen = ImageDataGenerator(rescale= 1./255)\n",
    "test_set = test_datagen.flow_from_directory(\n",
    "    'dataset/test_set',\n",
    "    target_size = (64,64),\n",
    "    batch_size = 32,\n",
    "    class_mode = 'categorical'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "CNvuC8xiRxj6"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "cnn = tf.keras.models.Sequential()\n",
    "# Sequential 모델은 층을 순차적으로 쌓아 올리는 단순한 모델 구조를 정의할 때 사용된다.\n",
    "# 순차적으로 층을 추가하고, 데이터가 각 층을 통과하며 변환되도록 한다.\n",
    "\n",
    "cnn.add(tf.keras.layers.Conv2D(filters=32, kernel_size =3, activation = 'relu', input_shape=(64,64,3)))\n",
    "cnn.add(tf.keras.layers.MaxPool2D(pool_size = 2, strides=2))\n",
    "\n",
    "cnn.add(tf.keras.layers.Conv2D(filters=32, kernel_size = 3, activation = 'relu'))\n",
    "cnn.add(tf.keras.layers.MaxPool2D(pool_size=2, strides=2))\n",
    "\n",
    "cnn.add(tf.keras.layers.Flatten())\n",
    "\n",
    "cnn.add(tf.keras.layers.Dense(units=128, activation='relu'))\n",
    "cnn.add(tf.keras.layers.Dense(units=9, activation='softmax')) # 클래스 수에 맞게 유닛 수를 9로 설정하고 softmax activate function 활성화 함수 사용\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "Bg7Js9y3R1TU"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 1s/step - accuracy: 0.2361 - loss: 2.0598 - val_accuracy: 0.2818 - val_loss: 1.9696\n",
      "Epoch 2/40\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 902ms/step - accuracy: 0.3350 - loss: 1.8676 - val_accuracy: 0.3333 - val_loss: 1.9870\n",
      "Epoch 3/40\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 936ms/step - accuracy: 0.4183 - loss: 1.7013 - val_accuracy: 0.3550 - val_loss: 2.0487\n",
      "Epoch 4/40\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 932ms/step - accuracy: 0.5175 - loss: 1.4770 - val_accuracy: 0.2276 - val_loss: 2.5342\n",
      "Epoch 5/40\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 828ms/step - accuracy: 0.5717 - loss: 1.3283 - val_accuracy: 0.3659 - val_loss: 2.2412\n",
      "Epoch 6/40\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 557ms/step - accuracy: 0.5937 - loss: 1.1158 - val_accuracy: 0.3984 - val_loss: 2.0544\n",
      "Epoch 7/40\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 556ms/step - accuracy: 0.6349 - loss: 1.0183 - val_accuracy: 0.3388 - val_loss: 2.4546\n",
      "Epoch 8/40\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 553ms/step - accuracy: 0.7138 - loss: 0.8254 - val_accuracy: 0.2412 - val_loss: 3.3195\n",
      "Epoch 9/40\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 553ms/step - accuracy: 0.6883 - loss: 0.8880 - val_accuracy: 0.3062 - val_loss: 2.6009\n",
      "Epoch 10/40\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 559ms/step - accuracy: 0.7346 - loss: 0.8106 - val_accuracy: 0.3577 - val_loss: 2.8144\n",
      "Epoch 11/40\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 556ms/step - accuracy: 0.7934 - loss: 0.6478 - val_accuracy: 0.3388 - val_loss: 3.1131\n",
      "Epoch 12/40\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 560ms/step - accuracy: 0.8203 - loss: 0.5609 - val_accuracy: 0.3550 - val_loss: 2.9198\n",
      "Epoch 13/40\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 557ms/step - accuracy: 0.8551 - loss: 0.4728 - val_accuracy: 0.3469 - val_loss: 3.1830\n",
      "Epoch 14/40\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 560ms/step - accuracy: 0.8385 - loss: 0.4773 - val_accuracy: 0.3577 - val_loss: 3.1367\n",
      "Epoch 15/40\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 576ms/step - accuracy: 0.8460 - loss: 0.4860 - val_accuracy: 0.3523 - val_loss: 3.1308\n",
      "Epoch 16/40\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 565ms/step - accuracy: 0.8685 - loss: 0.4794 - val_accuracy: 0.3659 - val_loss: 3.5677\n",
      "Epoch 17/40\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 569ms/step - accuracy: 0.8468 - loss: 0.4461 - val_accuracy: 0.3144 - val_loss: 3.6459\n",
      "Epoch 18/40\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 578ms/step - accuracy: 0.8999 - loss: 0.3359 - val_accuracy: 0.3984 - val_loss: 3.2182\n",
      "Epoch 19/40\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 563ms/step - accuracy: 0.9052 - loss: 0.3061 - val_accuracy: 0.3767 - val_loss: 3.5160\n",
      "Epoch 20/40\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 567ms/step - accuracy: 0.8950 - loss: 0.3158 - val_accuracy: 0.3225 - val_loss: 4.0739\n",
      "Epoch 21/40\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 565ms/step - accuracy: 0.9226 - loss: 0.2775 - val_accuracy: 0.3604 - val_loss: 3.6762\n",
      "Epoch 22/40\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 561ms/step - accuracy: 0.9043 - loss: 0.2905 - val_accuracy: 0.3984 - val_loss: 3.5355\n",
      "Epoch 23/40\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 555ms/step - accuracy: 0.9213 - loss: 0.2526 - val_accuracy: 0.3198 - val_loss: 4.2531\n",
      "Epoch 24/40\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 563ms/step - accuracy: 0.9130 - loss: 0.2641 - val_accuracy: 0.3144 - val_loss: 5.0007\n",
      "Epoch 25/40\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 562ms/step - accuracy: 0.9510 - loss: 0.1988 - val_accuracy: 0.3902 - val_loss: 3.7085\n",
      "Epoch 26/40\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 559ms/step - accuracy: 0.9310 - loss: 0.2051 - val_accuracy: 0.3333 - val_loss: 4.1381\n",
      "Epoch 27/40\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 555ms/step - accuracy: 0.9278 - loss: 0.2648 - val_accuracy: 0.3469 - val_loss: 4.3080\n",
      "Epoch 28/40\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 556ms/step - accuracy: 0.9426 - loss: 0.1861 - val_accuracy: 0.3550 - val_loss: 4.5808\n",
      "Epoch 29/40\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 562ms/step - accuracy: 0.9225 - loss: 0.2082 - val_accuracy: 0.3469 - val_loss: 4.4635\n",
      "Epoch 30/40\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 571ms/step - accuracy: 0.9557 - loss: 0.1471 - val_accuracy: 0.3388 - val_loss: 4.8876\n",
      "Epoch 31/40\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 567ms/step - accuracy: 0.9399 - loss: 0.1622 - val_accuracy: 0.3631 - val_loss: 4.7281\n",
      "Epoch 32/40\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 563ms/step - accuracy: 0.9408 - loss: 0.1729 - val_accuracy: 0.3848 - val_loss: 5.0451\n",
      "Epoch 33/40\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 670ms/step - accuracy: 0.9497 - loss: 0.1163 - val_accuracy: 0.3604 - val_loss: 4.8838\n",
      "Epoch 34/40\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 578ms/step - accuracy: 0.9605 - loss: 0.1246 - val_accuracy: 0.3415 - val_loss: 5.5696\n",
      "Epoch 35/40\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 586ms/step - accuracy: 0.9569 - loss: 0.1657 - val_accuracy: 0.4201 - val_loss: 4.8737\n",
      "Epoch 36/40\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 593ms/step - accuracy: 0.9508 - loss: 0.1366 - val_accuracy: 0.3930 - val_loss: 4.8140\n",
      "Epoch 37/40\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2214s\u001b[0m 740ms/step - accuracy: 0.9406 - loss: 0.1609 - val_accuracy: 0.3875 - val_loss: 4.6312\n",
      "Epoch 38/40\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 970ms/step - accuracy: 0.9575 - loss: 0.1441 - val_accuracy: 0.3875 - val_loss: 5.0899\n",
      "Epoch 39/40\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 995ms/step - accuracy: 0.9679 - loss: 0.1038 - val_accuracy: 0.3984 - val_loss: 4.9232\n",
      "Epoch 40/40\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 935ms/step - accuracy: 0.9509 - loss: 0.1598 - val_accuracy: 0.3550 - val_loss: 5.2333\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x1ccd58d00e0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Adam 옵티마이저에 학습률 지정\n",
    "\n",
    "learning_rate = 0.001\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate = learning_rate)\n",
    "\n",
    "cnn.compile(optimizer= optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "cnn.fit( x = train_set, validation_data = test_set, epochs=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "1fKaf70pR2zl"
   },
   "outputs": [],
   "source": [
    "\n",
    "# 테스트 이미지 예측 (class 9개 모두 다 해봐야 함)\n",
    "test_image1 = image.load_img ('dataset/single_prediction/PE드럼_파손.jpg', target_size=(64,64))\n",
    "\n",
    "\n",
    "# PIL (Python Imaging Libary) 이미지 인스턴스 = 이미지 데이터를 다루기 위한 python 객체\n",
    "# PIL을 넘파이 배열로 변환하기 왜냐? 신경망 모델 자체가 넘파이 배열을 입력으로 받기 때문이다~ PIL 이미지를 넘파이 배열로 변환해야 한다.\n",
    "\n",
    "\n",
    "test_image1 = image.img_to_array(test_image1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "epOcCt9DR4a3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "{'PE드럼 파손': 0, 'PE방호벽 파손': 1, 'PE안내봉 파손': 2, 'PE입간판 파손': 3, 'PE휀스 파손': 4, '도로 갈라짐': 5, '라바콘 파손': 6, '시선유도봉 파손': 7, '제설함 파손': 8}\n",
      "PE드럼 파손\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 배열에 새로운 차원을 추가한다.\n",
    "# 딥러닝 프레임워크에서 모델이 입력 데이터를 처리하는 방식이 그렇다. keras, tensorflow, pytorch 등등 모두 데이터를 배치 단위로 처리한다.\n",
    "# 4차원 입력 배열의 필요성\n",
    "# 딥러닝 모델에 입력되는 데이터의 형식 : (배치크기, 높이, 너비, 채널수)\n",
    "# 배치 처리를 위해서 배열을 4차원으로 만들기 위해 사용된다. (배치 크기, 높이, 너비, 채널 수)\n",
    "\n",
    "test_image1 = np.expand_dims(test_image1, axis=0)\n",
    "\n",
    "result1 = cnn.predict(test_image1/255.0) # test image도 픽셀 값이 0부터 255 사이의 값이기 때문에, 이걸 정규화 해줘야 한다. 0부터 1 사이의 값으로 정규화 실시한다.\n",
    "# result = (1,9) 1개의 이미지와 9개의 클래스\n",
    "\n",
    "# 클래스 인덱스 확인하기\n",
    "\n",
    "print(train_set.class_indices)  # class_indices 속성: 클래스 레이블과 디렉토리 이름 사이의 매핑을 반환한다.\n",
    "\n",
    "# 예측 결과 출력하기\n",
    "\n",
    "predicted_class = np.argmax(result1, axis=1) # 하나의 요소를 가진 배열이 된다.\n",
    "class_labels = list(train_set.class_indices.keys()) # 딕셔너리의 key (pe드럼 파손, pe휀스 파손 등등 ... ) 만 추출해서 list 로 변환한다.\n",
    "prediction = class_labels[predicted_class[0]] # predicted_class의 첫번째이자 유일한 요소를 가져온다.\n",
    "\n",
    "print(prediction)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "xM86fkbWR7qW"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "{'PE드럼 파손': 0, 'PE방호벽 파손': 1, 'PE안내봉 파손': 2, 'PE입간판 파손': 3, 'PE휀스 파손': 4, '도로 갈라짐': 5, '라바콘 파손': 6, '시선유도봉 파손': 7, '제설함 파손': 8}\n",
      "PE방호벽 파손\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 테스트 이미지 예측 (class 9개 모두 다 해봐야 함)\n",
    "test_image1 = image.load_img ('dataset/single_prediction/PE방호벽_파손.jpg', target_size=(64,64))\n",
    "\n",
    "\n",
    "# PIL (Python Imaging Libary) 이미지 인스턴스 = 이미지 데이터를 다루기 위한 python 객체\n",
    "# PIL을 넘파이 배열로 변환하기 왜냐? 신경망 모델 자체가 넘파이 배열을 입력으로 받기 때문이다~ PIL 이미지를 넘파이 배열로 변환해야 한다.\n",
    "\n",
    "\n",
    "test_image1 = image.img_to_array(test_image1)\n",
    "\n",
    "\n",
    "# 배열에 새로운 차원을 추가한다.\n",
    "# 딥러닝 프레임워크에서 모델이 입력 데이터를 처리하는 방식이 그렇다. keras, tensorflow, pytorch 등등 모두 데이터를 배치 단위로 처리한다.\n",
    "# 4차원 입력 배열의 필요성\n",
    "# 딥러닝 모델에 입력되는 데이터의 형식 : (배치크기, 높이, 너비, 채널수)\n",
    "# 배치 처리를 위해서 배열을 4차원으로 만들기 위해 사용된다. (배치 크기, 높이, 너비, 채널 수)\n",
    "\n",
    "test_image1 = np.expand_dims(test_image1, axis=0)\n",
    "\n",
    "result1 = cnn.predict(test_image1/255.0) # test image도 픽셀 값이 0부터 255 사이의 값이기 때문에, 이걸 정규화 해줘야 한다. 0부터 1 사이의 값으로 정규화 실시한다.\n",
    "# result = (1,9) 1개의 이미지와 9개의 클래스\n",
    "\n",
    "# 클래스 인덱스 확인하기\n",
    "\n",
    "print(train_set.class_indices)  # class_indices 속성: 클래스 레이블과 디렉토리 이름 사이의 매핑을 반환한다.\n",
    "\n",
    "# 예측 결과 출력하기\n",
    "\n",
    "predicted_class = np.argmax(result1, axis=1) # 하나의 요소를 가진 배열이 된다.\n",
    "class_labels = list(train_set.class_indices.keys()) # 딕셔너리의 key (pe드럼 파손, pe휀스 파손 등등 ... ) 만 추출해서 list 로 변환한다.\n",
    "prediction = class_labels[predicted_class[0]] # predicted_class의 첫번째이자 유일한 요소를 가져온다.\n",
    "\n",
    "print(prediction)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "{'PE드럼 파손': 0, 'PE방호벽 파손': 1, 'PE안내봉 파손': 2, 'PE입간판 파손': 3, 'PE휀스 파손': 4, '도로 갈라짐': 5, '라바콘 파손': 6, '시선유도봉 파손': 7, '제설함 파손': 8}\n",
      "PE안내봉 파손\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 테스트 이미지 예측 (class 9개 모두 다 해봐야 함)\n",
    "test_image1 = image.load_img ('dataset/single_prediction/PE안내봉_파손.jpg', target_size=(64,64))\n",
    "\n",
    "\n",
    "# PIL (Python Imaging Libary) 이미지 인스턴스 = 이미지 데이터를 다루기 위한 python 객체\n",
    "# PIL을 넘파이 배열로 변환하기 왜냐? 신경망 모델 자체가 넘파이 배열을 입력으로 받기 때문이다~ PIL 이미지를 넘파이 배열로 변환해야 한다.\n",
    "\n",
    "\n",
    "test_image1 = image.img_to_array(test_image1)\n",
    "\n",
    "\n",
    "# 배열에 새로운 차원을 추가한다.\n",
    "# 딥러닝 프레임워크에서 모델이 입력 데이터를 처리하는 방식이 그렇다. keras, tensorflow, pytorch 등등 모두 데이터를 배치 단위로 처리한다.\n",
    "# 4차원 입력 배열의 필요성\n",
    "# 딥러닝 모델에 입력되는 데이터의 형식 : (배치크기, 높이, 너비, 채널수)\n",
    "# 배치 처리를 위해서 배열을 4차원으로 만들기 위해 사용된다. (배치 크기, 높이, 너비, 채널 수)\n",
    "\n",
    "test_image1 = np.expand_dims(test_image1, axis=0)\n",
    "\n",
    "result1 = cnn.predict(test_image1/255.0) # test image도 픽셀 값이 0부터 255 사이의 값이기 때문에, 이걸 정규화 해줘야 한다. 0부터 1 사이의 값으로 정규화 실시한다.\n",
    "# result = (1,9) 1개의 이미지와 9개의 클래스\n",
    "\n",
    "# 클래스 인덱스 확인하기\n",
    "\n",
    "print(train_set.class_indices)  # class_indices 속성: 클래스 레이블과 디렉토리 이름 사이의 매핑을 반환한다.\n",
    "\n",
    "# 예측 결과 출력하기\n",
    "\n",
    "predicted_class = np.argmax(result1, axis=1) # 하나의 요소를 가진 배열이 된다.\n",
    "class_labels = list(train_set.class_indices.keys()) # 딕셔너리의 key (pe드럼 파손, pe휀스 파손 등등 ... ) 만 추출해서 list 로 변환한다.\n",
    "prediction = class_labels[predicted_class[0]] # predicted_class의 첫번째이자 유일한 요소를 가져온다.\n",
    "\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "{'PE드럼 파손': 0, 'PE방호벽 파손': 1, 'PE안내봉 파손': 2, 'PE입간판 파손': 3, 'PE휀스 파손': 4, '도로 갈라짐': 5, '라바콘 파손': 6, '시선유도봉 파손': 7, '제설함 파손': 8}\n",
      "도로 갈라짐\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 테스트 이미지 예측 (class 9개 모두 다 해봐야 함)\n",
    "test_image1 = image.load_img ('dataset/single_prediction/도로_갈라짐.jpg', target_size=(64,64))\n",
    "\n",
    "\n",
    "# PIL (Python Imaging Libary) 이미지 인스턴스 = 이미지 데이터를 다루기 위한 python 객체\n",
    "# PIL을 넘파이 배열로 변환하기 왜냐? 신경망 모델 자체가 넘파이 배열을 입력으로 받기 때문이다~ PIL 이미지를 넘파이 배열로 변환해야 한다.\n",
    "\n",
    "\n",
    "test_image1 = image.img_to_array(test_image1)\n",
    "\n",
    "\n",
    "# 배열에 새로운 차원을 추가한다.\n",
    "# 딥러닝 프레임워크에서 모델이 입력 데이터를 처리하는 방식이 그렇다. keras, tensorflow, pytorch 등등 모두 데이터를 배치 단위로 처리한다.\n",
    "# 4차원 입력 배열의 필요성\n",
    "# 딥러닝 모델에 입력되는 데이터의 형식 : (배치크기, 높이, 너비, 채널수)\n",
    "# 배치 처리를 위해서 배열을 4차원으로 만들기 위해 사용된다. (배치 크기, 높이, 너비, 채널 수)\n",
    "\n",
    "test_image1 = np.expand_dims(test_image1, axis=0)\n",
    "\n",
    "result1 = cnn.predict(test_image1/255.0) # test image도 픽셀 값이 0부터 255 사이의 값이기 때문에, 이걸 정규화 해줘야 한다. 0부터 1 사이의 값으로 정규화 실시한다.\n",
    "# result = (1,9) 1개의 이미지와 9개의 클래스\n",
    "\n",
    "# 클래스 인덱스 확인하기\n",
    "\n",
    "print(train_set.class_indices)  # class_indices 속성: 클래스 레이블과 디렉토리 이름 사이의 매핑을 반환한다.\n",
    "\n",
    "# 예측 결과 출력하기\n",
    "\n",
    "predicted_class = np.argmax(result1, axis=1) # 하나의 요소를 가진 배열이 된다.\n",
    "class_labels = list(train_set.class_indices.keys()) # 딕셔너리의 key (pe드럼 파손, pe휀스 파손 등등 ... ) 만 추출해서 list 로 변환한다.\n",
    "prediction = class_labels[predicted_class[0]] # predicted_class의 첫번째이자 유일한 요소를 가져온다.\n",
    "\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "{'PE드럼 파손': 0, 'PE방호벽 파손': 1, 'PE안내봉 파손': 2, 'PE입간판 파손': 3, 'PE휀스 파손': 4, '도로 갈라짐': 5, '라바콘 파손': 6, '시선유도봉 파손': 7, '제설함 파손': 8}\n",
      "라바콘 파손\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 테스트 이미지 예측 (class 9개 모두 다 해봐야 함)\n",
    "test_image1 = image.load_img ('dataset/single_prediction/라바콘_파손.jpg', target_size=(64,64))\n",
    "\n",
    "\n",
    "# PIL (Python Imaging Libary) 이미지 인스턴스 = 이미지 데이터를 다루기 위한 python 객체\n",
    "# PIL을 넘파이 배열로 변환하기 왜냐? 신경망 모델 자체가 넘파이 배열을 입력으로 받기 때문이다~ PIL 이미지를 넘파이 배열로 변환해야 한다.\n",
    "\n",
    "\n",
    "test_image1 = image.img_to_array(test_image1)\n",
    "\n",
    "\n",
    "# 배열에 새로운 차원을 추가한다.\n",
    "# 딥러닝 프레임워크에서 모델이 입력 데이터를 처리하는 방식이 그렇다. keras, tensorflow, pytorch 등등 모두 데이터를 배치 단위로 처리한다.\n",
    "# 4차원 입력 배열의 필요성\n",
    "# 딥러닝 모델에 입력되는 데이터의 형식 : (배치크기, 높이, 너비, 채널수)\n",
    "# 배치 처리를 위해서 배열을 4차원으로 만들기 위해 사용된다. (배치 크기, 높이, 너비, 채널 수)\n",
    "\n",
    "test_image1 = np.expand_dims(test_image1, axis=0)\n",
    "\n",
    "result1 = cnn.predict(test_image1/255.0) # test image도 픽셀 값이 0부터 255 사이의 값이기 때문에, 이걸 정규화 해줘야 한다. 0부터 1 사이의 값으로 정규화 실시한다.\n",
    "# result = (1,9) 1개의 이미지와 9개의 클래스\n",
    "\n",
    "# 클래스 인덱스 확인하기\n",
    "\n",
    "print(train_set.class_indices)  # class_indices 속성: 클래스 레이블과 디렉토리 이름 사이의 매핑을 반환한다.\n",
    "\n",
    "# 예측 결과 출력하기\n",
    "\n",
    "predicted_class = np.argmax(result1, axis=1) # 하나의 요소를 가진 배열이 된다.\n",
    "class_labels = list(train_set.class_indices.keys()) # 딕셔너리의 key (pe드럼 파손, pe휀스 파손 등등 ... ) 만 추출해서 list 로 변환한다.\n",
    "prediction = class_labels[predicted_class[0]] # predicted_class의 첫번째이자 유일한 요소를 가져온다.\n",
    "\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "{'PE드럼 파손': 0, 'PE방호벽 파손': 1, 'PE안내봉 파손': 2, 'PE입간판 파손': 3, 'PE휀스 파손': 4, '도로 갈라짐': 5, '라바콘 파손': 6, '시선유도봉 파손': 7, '제설함 파손': 8}\n",
      "시선유도봉 파손\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 테스트 이미지 예측 (class 9개 모두 다 해봐야 함)\n",
    "test_image1 = image.load_img ('dataset/single_prediction/시선유도봉_파손.jpg', target_size=(64,64))\n",
    "\n",
    "\n",
    "# PIL (Python Imaging Libary) 이미지 인스턴스 = 이미지 데이터를 다루기 위한 python 객체\n",
    "# PIL을 넘파이 배열로 변환하기 왜냐? 신경망 모델 자체가 넘파이 배열을 입력으로 받기 때문이다~ PIL 이미지를 넘파이 배열로 변환해야 한다.\n",
    "\n",
    "\n",
    "test_image1 = image.img_to_array(test_image1)\n",
    "\n",
    "\n",
    "# 배열에 새로운 차원을 추가한다.\n",
    "# 딥러닝 프레임워크에서 모델이 입력 데이터를 처리하는 방식이 그렇다. keras, tensorflow, pytorch 등등 모두 데이터를 배치 단위로 처리한다.\n",
    "# 4차원 입력 배열의 필요성\n",
    "# 딥러닝 모델에 입력되는 데이터의 형식 : (배치크기, 높이, 너비, 채널수)\n",
    "# 배치 처리를 위해서 배열을 4차원으로 만들기 위해 사용된다. (배치 크기, 높이, 너비, 채널 수)\n",
    "\n",
    "test_image1 = np.expand_dims(test_image1, axis=0)\n",
    "\n",
    "result1 = cnn.predict(test_image1/255.0) # test image도 픽셀 값이 0부터 255 사이의 값이기 때문에, 이걸 정규화 해줘야 한다. 0부터 1 사이의 값으로 정규화 실시한다.\n",
    "# result = (1,9) 1개의 이미지와 9개의 클래스\n",
    "\n",
    "# 클래스 인덱스 확인하기\n",
    "\n",
    "print(train_set.class_indices)  # class_indices 속성: 클래스 레이블과 디렉토리 이름 사이의 매핑을 반환한다.\n",
    "\n",
    "# 예측 결과 출력하기\n",
    "\n",
    "predicted_class = np.argmax(result1, axis=1) # 하나의 요소를 가진 배열이 된다.\n",
    "class_labels = list(train_set.class_indices.keys()) # 딕셔너리의 key (pe드럼 파손, pe휀스 파손 등등 ... ) 만 추출해서 list 로 변환한다.\n",
    "prediction = class_labels[predicted_class[0]] # predicted_class의 첫번째이자 유일한 요소를 가져온다.\n",
    "\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "{'PE드럼 파손': 0, 'PE방호벽 파손': 1, 'PE안내봉 파손': 2, 'PE입간판 파손': 3, 'PE휀스 파손': 4, '도로 갈라짐': 5, '라바콘 파손': 6, '시선유도봉 파손': 7, '제설함 파손': 8}\n",
      "제설함 파손\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 테스트 이미지 예측 (class 9개 모두 다 해봐야 함)\n",
    "test_image1 = image.load_img ('dataset/single_prediction/제설함_파손.jpg', target_size=(64,64))\n",
    "\n",
    "\n",
    "# PIL (Python Imaging Libary) 이미지 인스턴스 = 이미지 데이터를 다루기 위한 python 객체\n",
    "# PIL을 넘파이 배열로 변환하기 왜냐? 신경망 모델 자체가 넘파이 배열을 입력으로 받기 때문이다~ PIL 이미지를 넘파이 배열로 변환해야 한다.\n",
    "\n",
    "\n",
    "test_image1 = image.img_to_array(test_image1)\n",
    "\n",
    "\n",
    "# 배열에 새로운 차원을 추가한다.\n",
    "# 딥러닝 프레임워크에서 모델이 입력 데이터를 처리하는 방식이 그렇다. keras, tensorflow, pytorch 등등 모두 데이터를 배치 단위로 처리한다.\n",
    "# 4차원 입력 배열의 필요성\n",
    "# 딥러닝 모델에 입력되는 데이터의 형식 : (배치크기, 높이, 너비, 채널수)\n",
    "# 배치 처리를 위해서 배열을 4차원으로 만들기 위해 사용된다. (배치 크기, 높이, 너비, 채널 수)\n",
    "\n",
    "test_image1 = np.expand_dims(test_image1, axis=0)\n",
    "\n",
    "result1 = cnn.predict(test_image1/255.0) # test image도 픽셀 값이 0부터 255 사이의 값이기 때문에, 이걸 정규화 해줘야 한다. 0부터 1 사이의 값으로 정규화 실시한다.\n",
    "# result = (1,9) 1개의 이미지와 9개의 클래스\n",
    "\n",
    "# 클래스 인덱스 확인하기\n",
    "\n",
    "print(train_set.class_indices)  # class_indices 속성: 클래스 레이블과 디렉토리 이름 사이의 매핑을 반환한다.\n",
    "\n",
    "# 예측 결과 출력하기\n",
    "\n",
    "predicted_class = np.argmax(result1, axis=1) # 하나의 요소를 가진 배열이 된다.\n",
    "class_labels = list(train_set.class_indices.keys()) # 딕셔너리의 key (pe드럼 파손, pe휀스 파손 등등 ... ) 만 추출해서 list 로 변환한다.\n",
    "prediction = class_labels[predicted_class[0]] # predicted_class의 첫번째이자 유일한 요소를 가져온다.\n",
    "\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "{'PE드럼 파손': 0, 'PE방호벽 파손': 1, 'PE안내봉 파손': 2, 'PE입간판 파손': 3, 'PE휀스 파손': 4, '도로 갈라짐': 5, '라바콘 파손': 6, '시선유도봉 파손': 7, '제설함 파손': 8}\n",
      "PE입간판 파손\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 테스트 이미지 예측 (class 9개 모두 다 해봐야 함)\n",
    "test_image1 = image.load_img ('dataset/single_prediction/PE입간판_파손.png', target_size=(64,64))\n",
    "\n",
    "\n",
    "# PIL (Python Imaging Libary) 이미지 인스턴스 = 이미지 데이터를 다루기 위한 python 객체\n",
    "# PIL을 넘파이 배열로 변환하기 왜냐? 신경망 모델 자체가 넘파이 배열을 입력으로 받기 때문이다~ PIL 이미지를 넘파이 배열로 변환해야 한다.\n",
    "\n",
    "\n",
    "test_image1 = image.img_to_array(test_image1)\n",
    "\n",
    "\n",
    "# 배열에 새로운 차원을 추가한다.\n",
    "# 딥러닝 프레임워크에서 모델이 입력 데이터를 처리하는 방식이 그렇다. keras, tensorflow, pytorch 등등 모두 데이터를 배치 단위로 처리한다.\n",
    "# 4차원 입력 배열의 필요성\n",
    "# 딥러닝 모델에 입력되는 데이터의 형식 : (배치크기, 높이, 너비, 채널수)\n",
    "# 배치 처리를 위해서 배열을 4차원으로 만들기 위해 사용된다. (배치 크기, 높이, 너비, 채널 수)\n",
    "\n",
    "test_image1 = np.expand_dims(test_image1, axis=0)\n",
    "\n",
    "result1 = cnn.predict(test_image1/255.0) # test image도 픽셀 값이 0부터 255 사이의 값이기 때문에, 이걸 정규화 해줘야 한다. 0부터 1 사이의 값으로 정규화 실시한다.\n",
    "# result = (1,9) 1개의 이미지와 9개의 클래스\n",
    "\n",
    "# 클래스 인덱스 확인하기\n",
    "\n",
    "print(train_set.class_indices)  # class_indices 속성: 클래스 레이블과 디렉토리 이름 사이의 매핑을 반환한다.\n",
    "\n",
    "# 예측 결과 출력하기\n",
    "\n",
    "predicted_class = np.argmax(result1, axis=1) # 하나의 요소를 가진 배열이 된다.\n",
    "class_labels = list(train_set.class_indices.keys()) # 딕셔너리의 key (pe드럼 파손, pe휀스 파손 등등 ... ) 만 추출해서 list 로 변환한다.\n",
    "prediction = class_labels[predicted_class[0]] # predicted_class의 첫번째이자 유일한 요소를 가져온다.\n",
    "\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "# CNN 모델 저장하고 TFLITE 로 변환하기\n",
    "\n",
    "# 1. 모델 저장\n",
    "cnn.save('cnn_krs.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#2. TFLite로 변환\n",
    "\n",
    "import tensorflow as tf \n",
    "\n",
    "# 모델 로드하기 \n",
    "\n",
    "cnn_keras_model = tf.keras.models.load_model('cnn_krs.h5')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Sequential' object has no attribute '_get_save_spec'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[94], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#TFLiteConverter를 사용하여 모델을 TFLite 형식으로 변환\u001b[39;00m\n\u001b[0;32m      3\u001b[0m converter \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mlite\u001b[38;5;241m.\u001b[39mTFLiteConverter\u001b[38;5;241m.\u001b[39mfrom_keras_model(cnn_keras_model)\n\u001b[1;32m----> 4\u001b[0m tf_cnnMdl \u001b[38;5;241m=\u001b[39m converter\u001b[38;5;241m.\u001b[39mconvert()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\lite\\python\\lite.py:1175\u001b[0m, in \u001b[0;36m_export_metrics.<locals>.wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1172\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(convert_func)\n\u001b[0;32m   1173\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m   1174\u001b[0m   \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m-> 1175\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_convert_and_export_metrics(convert_func, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\lite\\python\\lite.py:1129\u001b[0m, in \u001b[0;36mTFLiteConverterBase._convert_and_export_metrics\u001b[1;34m(self, convert_func, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1127\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_save_conversion_params_metric()\n\u001b[0;32m   1128\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mprocess_time()\n\u001b[1;32m-> 1129\u001b[0m result \u001b[38;5;241m=\u001b[39m convert_func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1130\u001b[0m elapsed_time_ms \u001b[38;5;241m=\u001b[39m (time\u001b[38;5;241m.\u001b[39mprocess_time() \u001b[38;5;241m-\u001b[39m start_time) \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m1000\u001b[39m\n\u001b[0;32m   1131\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m result:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\lite\\python\\lite.py:1641\u001b[0m, in \u001b[0;36mTFLiteKerasModelConverterV2.convert\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1637\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m saved_model_convert_result:\n\u001b[0;32m   1638\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m saved_model_convert_result\n\u001b[0;32m   1640\u001b[0m graph_def, input_tensors, output_tensors, frozen_func \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m-> 1641\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_freeze_keras_model()\n\u001b[0;32m   1642\u001b[0m )\n\u001b[0;32m   1644\u001b[0m graph_def \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimize_tf_model(\n\u001b[0;32m   1645\u001b[0m     graph_def, input_tensors, output_tensors, frozen_func\n\u001b[0;32m   1646\u001b[0m )\n\u001b[0;32m   1648\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m(TFLiteKerasModelConverterV2, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mconvert(\n\u001b[0;32m   1649\u001b[0m     graph_def, input_tensors, output_tensors\n\u001b[0;32m   1650\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\lite\\python\\convert_phase.py:215\u001b[0m, in \u001b[0;36mconvert_phase.<locals>.actual_decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    213\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m error:\n\u001b[0;32m    214\u001b[0m   report_error_message(\u001b[38;5;28mstr\u001b[39m(error))\n\u001b[1;32m--> 215\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m error \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\lite\\python\\convert_phase.py:205\u001b[0m, in \u001b[0;36mconvert_phase.<locals>.actual_decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    202\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    203\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    204\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 205\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    206\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m ConverterError \u001b[38;5;28;01mas\u001b[39;00m converter_error:\n\u001b[0;32m    207\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m converter_error\u001b[38;5;241m.\u001b[39merrors:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\lite\\python\\lite.py:1582\u001b[0m, in \u001b[0;36mTFLiteKerasModelConverterV2._freeze_keras_model\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1573\u001b[0m \u001b[38;5;66;03m# If the model's call is not a `tf.function`, then we need to first get its\u001b[39;00m\n\u001b[0;32m   1574\u001b[0m \u001b[38;5;66;03m# input signature from `model_input_signature` method. We can't directly\u001b[39;00m\n\u001b[0;32m   1575\u001b[0m \u001b[38;5;66;03m# call `trace_model_call` because otherwise the batch dimension is set\u001b[39;00m\n\u001b[0;32m   1576\u001b[0m \u001b[38;5;66;03m# to None.\u001b[39;00m\n\u001b[0;32m   1577\u001b[0m \u001b[38;5;66;03m# Once we have better support for dynamic shapes, we can remove this.\u001b[39;00m\n\u001b[0;32m   1578\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_keras_model\u001b[38;5;241m.\u001b[39mcall, _def_function\u001b[38;5;241m.\u001b[39mFunction):\n\u001b[0;32m   1579\u001b[0m   \u001b[38;5;66;03m# Pass `keep_original_batch_size=True` will ensure that we get an input\u001b[39;00m\n\u001b[0;32m   1580\u001b[0m   \u001b[38;5;66;03m# signature including the batch dimension specified by the user.\u001b[39;00m\n\u001b[0;32m   1581\u001b[0m   \u001b[38;5;66;03m# TODO(b/169898786): Use the Keras public API when TFLite moves out of TF\u001b[39;00m\n\u001b[1;32m-> 1582\u001b[0m   input_signature \u001b[38;5;241m=\u001b[39m _model_input_signature(\n\u001b[0;32m   1583\u001b[0m       \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_keras_model, keep_original_batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m   1584\u001b[0m   )\n\u001b[0;32m   1586\u001b[0m \u001b[38;5;66;03m# TODO(b/169898786): Use the Keras public API when TFLite moves out of TF\u001b[39;00m\n\u001b[0;32m   1587\u001b[0m func \u001b[38;5;241m=\u001b[39m _trace_model_call(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_keras_model, input_signature)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\lite\\python\\tflite_keras_util.py:84\u001b[0m, in \u001b[0;36mmodel_input_signature\u001b[1;34m(model, keep_original_batch_size)\u001b[0m\n\u001b[0;32m     82\u001b[0m   input_specs \u001b[38;5;241m=\u001b[39m input_specs[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     83\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 84\u001b[0m   input_specs \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39m_get_save_spec(  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m     85\u001b[0m       dynamic_batch\u001b[38;5;241m=\u001b[39m\u001b[38;5;129;01mnot\u001b[39;00m keep_original_batch_size)\n\u001b[0;32m     86\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m input_specs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     87\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Sequential' object has no attribute '_get_save_spec'"
     ]
    }
   ],
   "source": [
    "\n",
    "#TFLiteConverter를 사용하여 모델을 TFLite 형식으로 변환\n",
    "\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(cnn_keras_model)\n",
    "tf_cnnMdl = converter.convert()\n",
    "\n",
    "\n",
    "# TFLite 모델을 파일로 저장하기\n",
    "\n",
    "with open('cnn_keras_mdl.tflite', 'wb' ) as f:\n",
    "    f.write(tf_cnnMdl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
